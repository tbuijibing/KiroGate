# -*- coding: utf-8 -*-

# GeekGate
# Based on kiro-openai-gateway by Jwadow (https://github.com/Jwadow/kiro-openai-gateway)
# Original Copyright (C) 2025 Jwadow
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""
è¯·æ±‚å¤„ç†å…¬å…±å‡½æ•°?

æå– /v1/chat/completions ?/v1/messages ç«¯ç‚¹çš„å…¬å…±é€»è¾‘?
å‡å°‘ä»£ç é‡å¤ï¼Œæé«˜å¯ç»´æŠ¤æ€?
"""

import json
import time
from typing import Any, Callable, Dict, List, Optional, Union

from fastapi import HTTPException, Request, Response
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger

from geek_gateway.auth import GeekAuthManager
from geek_gateway.cache import ModelInfoCache
from geek_gateway.converters import build_kiro_payload, convert_anthropic_to_openai_request, is_thinking_enabled
from geek_gateway.config import settings
from geek_gateway.http_client import GeekHttpClient
from geek_gateway.models import (
    ChatCompletionRequest,
    AnthropicMessagesRequest,
)
from geek_gateway.streaming import (
    stream_kiro_to_openai,
    collect_stream_response,
    stream_kiro_to_anthropic,
    collect_anthropic_response,
)
from geek_gateway.utils import generate_conversation_id, get_kiro_headers
from geek_gateway.config import settings, AUTO_CHUNKING_ENABLED, AUTO_CHUNK_THRESHOLD
from geek_gateway.metrics import metrics


# å¯¼å…¥å¯é€‰çš„è‡ªåŠ¨åˆ†ç‰‡å¤„ç†?
try:
    from geek_gateway.auto_chunked_handler import process_with_auto_chunking
    auto_chunking_available = True
except ImportError:
    auto_chunking_available = False


# å¯¼å…¥ debug_logger
try:
    from geek_gateway.debug_logger import debug_logger
except ImportError:
    debug_logger = None


class RequestHandler:
    """
    è¯·æ±‚å¤„ç†å™¨åŸºç±»ï¼Œå°è£…å…¬å…±é€»è¾‘?
    """

    @staticmethod
    def prepare_request_logging(request_data: Union[ChatCompletionRequest, AnthropicMessagesRequest]) -> None:
        """
        å‡†å¤‡è¯·æ±‚æ—¥å¿—è®°å½•?

        Args:
            request_data: è¯·æ±‚æ•°æ®
        """
        if debug_logger:
            debug_logger.prepare_new_request()

        try:
            request_body = json.dumps(request_data.model_dump(), ensure_ascii=False, indent=2).encode('utf-8')
            if debug_logger:
                debug_logger.log_request_body(request_body)
        except Exception as e:
            logger.warning(f"Failed to log request body: {e}")

    @staticmethod
    def log_kiro_request(kiro_payload: dict) -> None:
        """
        è®°å½• Kiro è¯·æ±‚?

        Args:
            kiro_payload: Kiro è¯·æ±‚ payload
        """
        try:
            kiro_request_body = json.dumps(kiro_payload, ensure_ascii=False, indent=2).encode('utf-8')
            if debug_logger:
                debug_logger.log_kiro_request_body(kiro_request_body)
        except Exception as e:
            logger.warning(f"Failed to log Kiro request: {e}")

    @staticmethod
    async def handle_api_error(
        response,
        http_client: GeekHttpClient,
        endpoint_name: str,
        error_format: str = "openai",
        request: Optional[Request] = None
    ) -> JSONResponse:
        """
        å¤„ç† API é”™è¯¯?

        Args:
            response: HTTP å“åº”
            http_client: HTTP å®¢æˆ·?
            endpoint_name: ç«¯ç‚¹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            error_format: é”™è¯¯æ ¼å¼?openai" ã€?anthropic"?

        Returns:
            JSONResponse é”™è¯¯å“åº”
        """
        try:
            error_content = await response.aread()
        except Exception:
            error_content = "æœªçŸ¥é”™è¯¯".encode("utf-8")
        finally:
            try:
                await response.aclose()
            except Exception:
                pass

        await http_client.close()
        error_text = error_content.decode('utf-8', errors='replace')
        logger.error(f"Error from Kiro API: {response.status_code} - {error_text}")

        # å°è¯•è§£æ JSON é”™è¯¯å“åº”
        error_message = error_text
        error_reason = None
        try:
            error_json = json.loads(error_text)
            if isinstance(error_json, dict):
                if "reason" in error_json:
                    error_reason = str(error_json["reason"])
                if "message" in error_json:
                    error_message = error_json["message"]
                elif "error" in error_json and isinstance(error_json["error"], dict):
                    if "message" in error_json["error"]:
                        error_message = error_json["error"]["message"]
                    if not error_reason and "reason" in error_json["error"]:
                        error_reason = str(error_json["error"]["reason"])
                if error_reason:
                    error_message = f"{error_message} (reason: {error_reason})"
        except (json.JSONDecodeError, KeyError):
            pass

        logger.warning(f"HTTP {response.status_code} - POST {endpoint_name} - {error_message[:100]}")

        if debug_logger:
            debug_logger.flush_on_error(response.status_code, error_message)

        if request and hasattr(request.state, "donated_token_id"):
            reason_text = error_reason or error_message
            if "MONTHLY_REQUEST_COUNT" in reason_text:
                try:
                    from geek_gateway.database import user_db
                    token_id = request.state.donated_token_id
                    user_db.set_token_status(token_id, "expired")
                    logger.warning(f"Token {token_id} marked expired due to monthly limit")
                except Exception as e:
                    logger.warning(f"Failed to mark token expired: {e}")

        # æ ¹æ®æ ¼å¼è¿”å›é”™è¯¯
        if error_format == "anthropic":
            return JSONResponse(
                status_code=response.status_code,
                content={
                    "type": "error",
                    "error": {
                        "type": "api_error",
                        "message": error_message
                    }
                }
            )
        else:
            return JSONResponse(
                status_code=response.status_code,
                content={
                    "error": {
                        "message": error_message,
                        "type": "kiro_api_error",
                        "code": response.status_code
                    }
                }
            )

    @staticmethod
    def log_success(endpoint_name: str, is_streaming: bool = False) -> None:
        """
        è®°å½•æˆåŠŸæ—¥å¿—?

        Args:
            endpoint_name: ç«¯ç‚¹åç§°
            is_streaming: æ˜¯å¦ä¸ºæµå¼å“?
        """
        mode = "streaming" if is_streaming else "non-streaming"
        logger.info(f"HTTP 200 - POST {endpoint_name} ({mode}) - completed")

    @staticmethod
    def log_error(endpoint_name: str, error: Union[str, Exception], status_code: int = 500) -> None:
        """
        è®°å½•é”™è¯¯æ—¥å¿—?

        Args:
            endpoint_name: ç«¯ç‚¹åç§°
            error: é”™è¯¯ä¿¡æ¯
            status_code: HTTP çŠ¶æ€ç 
        """
        if isinstance(error, Exception):
            error_msg = str(error) if str(error) else f"{type(error).__name__}: {repr(error)}"
        else:
            error_msg = error
        logger.error(f"HTTP {status_code} - POST {endpoint_name} - {error_msg[:100]}")

    @staticmethod
    def handle_streaming_error(error: Exception, endpoint_name: str) -> str:
        """
        å¤„ç†æµå¼é”™è¯¯ï¼Œç¡®ä¿é”™è¯¯ä¿¡æ¯ä¸ä¸ºç©º?

        Args:
            error: å¼‚å¸¸
            endpoint_name: ç«¯ç‚¹åç§°

        Returns:
            é”™è¯¯ä¿¡æ¯å­—ç¬¦?
        """
        error_msg = str(error) if str(error) else f"{type(error).__name__}: {repr(error)}"
        RequestHandler.log_error(endpoint_name, error_msg, 500)
        return error_msg

    @staticmethod
    def prepare_tokenizer_data(request_data: ChatCompletionRequest) -> tuple:
        """
        å‡†å¤‡ç”¨äº token è®¡æ•°çš„æ•°æ?

        Args:
            request_data: è¯·æ±‚æ•°æ®

        Returns:
            (messages_for_tokenizer, tools_for_tokenizer)
        """
        messages_for_tokenizer = [msg.model_dump() for msg in request_data.messages]
        tools_for_tokenizer = [tool.model_dump() for tool in request_data.tools] if request_data.tools else None
        return messages_for_tokenizer, tools_for_tokenizer

    @staticmethod
    async def create_stream_response(
        http_client: GeekHttpClient,
        response,
        model: str,
        model_cache: ModelInfoCache,
        auth_manager: GeekAuthManager,
        stream_func: Callable,
        endpoint_name: str,
        messages_for_tokenizer: Optional[List] = None,
        tools_for_tokenizer: Optional[List] = None,
        **kwargs
    ) -> StreamingResponse:
        """
        åˆ›å»ºæµå¼å“åº”?

        Args:
            http_client: HTTP å®¢æˆ·?
            response: Kiro API å“åº”
            model: æ¨¡å‹åç§°
            model_cache: æ¨¡å‹ç¼“å­˜
            auth_manager: è®¤è¯ç®¡ç†?
            stream_func: æµå¼å¤„ç†å‡½æ•°
            endpoint_name: ç«¯ç‚¹åç§°
            messages_for_tokenizer: æ¶ˆæ¯æ•°æ®ï¼ˆç”¨?token è®¡æ•°?
            tools_for_tokenizer: å·¥å…·æ•°æ®ï¼ˆç”¨?token è®¡æ•°?
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            StreamingResponse
        """
        async def stream_wrapper():
            streaming_error = None
            try:
                async for chunk in stream_func(
                    http_client.client,
                    response,
                    model,
                    model_cache,
                    auth_manager,
                    request_messages=messages_for_tokenizer,
                    request_tools=tools_for_tokenizer,
                    **kwargs
                ):
                    yield chunk
            except Exception as e:
                streaming_error = e
                raise
            finally:
                await http_client.close()
                if streaming_error:
                    RequestHandler.handle_streaming_error(streaming_error, endpoint_name)
                else:
                    RequestHandler.log_success(endpoint_name, is_streaming=True)
                if debug_logger:
                    if streaming_error:
                        error_msg = RequestHandler.handle_streaming_error(streaming_error, endpoint_name)
                        debug_logger.flush_on_error(500, error_msg)
                    else:
                        debug_logger.discard_buffers()

        return StreamingResponse(stream_wrapper(), media_type="text/event-stream")

    @staticmethod
    def should_enable_auto_chunking(messages: List) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨è‡ªåŠ¨åˆ†ç‰‡åŠŸèƒ?

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†ç‰‡
        """
        if not AUTO_CHUNKING_ENABLED or not auto_chunking_available:
            return False

        # æ£€æŸ¥æ¶ˆæ¯å†…å®¹æ˜¯å¦è¶…è¿‡é˜ˆ?
        total_chars = 0
        for msg in messages:
            if hasattr(msg, 'content'):
                content = msg.content
            elif isinstance(msg, dict):
                content = msg.get("content", "")
            else:
                continue

            if isinstance(content, str):
                total_chars += len(content)
            elif isinstance(content, list):
                for block in content:
                    if isinstance(block, dict) and block.get("type") == "text":
                        total_chars += len(block.get("text", ""))

        return total_chars > AUTO_CHUNK_THRESHOLD

    @staticmethod
    async def create_non_stream_response(
        http_client: GeekHttpClient,
        response,
        model: str,
        model_cache: ModelInfoCache,
        auth_manager: GeekAuthManager,
        collect_func: Callable,
        endpoint_name: str,
        messages_for_tokenizer: Optional[List] = None,
        tools_for_tokenizer: Optional[List] = None,
        **kwargs
    ) -> JSONResponse:
        """
        åˆ›å»ºéæµå¼å“åº?

        Args:
            http_client: HTTP å®¢æˆ·?
            response: Kiro API å“åº”
            model: æ¨¡å‹åç§°
            model_cache: æ¨¡å‹ç¼“å­˜
            auth_manager: è®¤è¯ç®¡ç†?
            collect_func: æ”¶é›†å“åº”å‡½æ•°
            endpoint_name: ç«¯ç‚¹åç§°
            messages_for_tokenizer: æ¶ˆæ¯æ•°æ®ï¼ˆç”¨?token è®¡æ•°?
            tools_for_tokenizer: å·¥å…·æ•°æ®ï¼ˆç”¨?token è®¡æ•°?
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            JSONResponse
        """
        collected_response = await collect_func(
            http_client.client,
            response,
            model,
            model_cache,
            auth_manager,
            request_messages=messages_for_tokenizer,
            request_tools=tools_for_tokenizer,
            **kwargs
        )

        await http_client.close()
        RequestHandler.log_success(endpoint_name, is_streaming=False)

        if debug_logger:
            debug_logger.discard_buffers()

        return JSONResponse(content=collected_response)

    @staticmethod
    async def process_request(
        request: Request,
        request_data: Union[ChatCompletionRequest, AnthropicMessagesRequest],
        endpoint_name: str,
        convert_to_openai: bool = False,
        response_format: str = "openai"
    ) -> Union[StreamingResponse, JSONResponse]:
        """
        å¤„ç†è¯·æ±‚çš„æ ¸å¿ƒé€»è¾‘?

        Args:
            request: FastAPI Request
            request_data: è¯·æ±‚æ•°æ®
            endpoint_name: ç«¯ç‚¹åç§°
            convert_to_openai: æ˜¯å¦éœ€è¦å°† Anthropic è¯·æ±‚è½¬æ¢?OpenAI æ ¼å¼
            response_format: å“åº”æ ¼å¼?openai" ã€?anthropic"?

        Returns:
            StreamingResponse ?JSONResponse
        """
        start_time = time.time()
        api_type = "anthropic" if response_format == "anthropic" else "openai"

        # Use auth_manager from request.state if available (multi-tenant mode)
        # Otherwise fall back to global auth_manager
        auth_manager: GeekAuthManager = getattr(request.state, 'auth_manager', None) or request.app.state.auth_manager
        model_cache: ModelInfoCache = request.app.state.model_cache

        # å‡†å¤‡æ—¥å¿—
        RequestHandler.prepare_request_logging(request_data)

        # å¦‚æœéœ€è¦ï¼Œè½¬æ¢ Anthropic è¯·æ±‚?OpenAI æ ¼å¼
        if convert_to_openai:
            try:
                openai_request = convert_anthropic_to_openai_request(request_data)
            except Exception as e:
                logger.error(f"Failed to convert Anthropic request: {e}")
                raise HTTPException(status_code=400, detail=f"è¯·æ±‚æ ¼å¼æ— æ•ˆ: {str(e)}")
        else:
            openai_request = request_data

        # ç”Ÿæˆä¼šè¯ ID
        conversation_id = generate_conversation_id()

        # è·å– thinking é…ç½®ï¼ˆä»åŸå§‹è¯·æ±‚ä¸­è·å–ï¼Œæ”¯æŒ Anthropic æ ¼å¼?
        thinking_config = getattr(request_data, 'thinking', None)
        thinking_enabled = is_thinking_enabled(thinking_config)

        # æ„å»º Kiro payload
        try:
            kiro_payload = build_kiro_payload(
                openai_request,
                conversation_id,
                auth_manager.profile_arn or "",
                thinking_config=thinking_config
            )
        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))

        # è®°å½• Kiro è¯·æ±‚
        RequestHandler.log_kiro_request(kiro_payload)

        # åˆ›å»º HTTP å®¢æˆ·?
        http_client = GeekHttpClient(auth_manager)
        url = f"{auth_manager.api_host}/generateAssistantResponse"

        try:
            # å‘é€è¯·æ±‚åˆ° Kiro API
            response = await http_client.request_with_retry(
                "POST",
                url,
                kiro_payload,
                stream=True,
                model=request_data.model
            )

            if response.status_code != 200:
                duration_ms = (time.time() - start_time) * 1000
                metrics.record_request(
                    endpoint=endpoint_name,
                    status_code=response.status_code,
                    duration_ms=duration_ms,
                    model=request_data.model,
                    is_stream=request_data.stream,
                    api_type=api_type
                )
                return await RequestHandler.handle_api_error(
                    response,
                    http_client,
                    endpoint_name,
                    response_format,
                    request=request
                )

            # å‡†å¤‡ token è®¡æ•°æ•°æ®
            messages_for_tokenizer, tools_for_tokenizer = RequestHandler.prepare_tokenizer_data(openai_request)

            # è®°å½•æˆåŠŸè¯·æ±‚
            duration_ms = (time.time() - start_time) * 1000
            metrics.record_request(
                endpoint=endpoint_name,
                status_code=200,
                duration_ms=duration_ms,
                model=request_data.model,
                is_stream=request_data.stream,
                api_type=api_type
            )

            # æ ¹æ®è¯·æ±‚ç±»å‹å’Œå“åº”æ ¼å¼å¤„?
            if request_data.stream:
                if response_format == "anthropic":
                    return await RequestHandler.create_stream_response(
                        http_client,
                        response,
                        request_data.model,
                        model_cache,
                        auth_manager,
                        stream_kiro_to_anthropic,
                        endpoint_name,
                        messages_for_tokenizer,
                        tools_for_tokenizer,
                        thinking_enabled=thinking_enabled
                    )
                else:
                    return await RequestHandler.create_stream_response(
                        http_client,
                        response,
                        request_data.model,
                        model_cache,
                        auth_manager,
                        stream_kiro_to_openai,
                        endpoint_name,
                        messages_for_tokenizer,
                        tools_for_tokenizer
                    )
            else:
                if response_format == "anthropic":
                    return await RequestHandler.create_non_stream_response(
                        http_client,
                        response,
                        request_data.model,
                        model_cache,
                        auth_manager,
                        collect_anthropic_response,
                        endpoint_name,
                        messages_for_tokenizer,
                        tools_for_tokenizer,
                        thinking_enabled=thinking_enabled
                    )
                else:
                    return await RequestHandler.create_non_stream_response(
                        http_client,
                        response,
                        request_data.model,
                        model_cache,
                        auth_manager,
                        collect_stream_response,
                        endpoint_name,
                        messages_for_tokenizer,
                        tools_for_tokenizer
                    )

        except HTTPException as e:
            await http_client.close()
            duration_ms = (time.time() - start_time) * 1000
            metrics.record_request(
                endpoint=endpoint_name,
                status_code=e.status_code,
                duration_ms=duration_ms,
                model=request_data.model,
                is_stream=request_data.stream,
                api_type=api_type
            )
            RequestHandler.log_error(endpoint_name, e.detail, e.status_code)
            if debug_logger:
                debug_logger.flush_on_error(e.status_code, str(e.detail))
            raise
        except Exception as e:
            await http_client.close()
            duration_ms = (time.time() - start_time) * 1000
            metrics.record_request(
                endpoint=endpoint_name,
                status_code=500,
                duration_ms=duration_ms,
                model=request_data.model,
                is_stream=request_data.stream,
                api_type=api_type
            )
            error_msg = str(e) if str(e) else f"{type(e).__name__}: {repr(e)}"
            logger.error(f"Internal error: {error_msg}", exc_info=True)
            RequestHandler.log_error(endpoint_name, error_msg, 500)
            if debug_logger:
                debug_logger.flush_on_error(500, error_msg)
            if settings.debug_mode == "off":
                detail = "æœåŠ¡å™¨å†…éƒ¨é”™è¯?
            else:
                detail = f"æœåŠ¡å™¨å†…éƒ¨é”™è¯? {error_msg}"
            raise HTTPException(status_code=500, detail=detail)
