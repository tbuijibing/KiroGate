# -*- coding: utf-8 -*-

# GeekGate
# Based on kiro-openai-gateway by Jwadow (https://github.com/Jwadow/kiro-openai-gateway)
# Original Copyright (C) 2025 Jwadow
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""
è‡ªåŠ¨åˆ†ç‰‡å¤„ç†å™?

å½“æ£€æµ‹åˆ°é•¿æ–‡æ¡£æ—¶ï¼Œè‡ªåŠ¨å°†å…¶åˆ†ç‰‡å¤„ç†ï¼Œæµå¼è¿”å›ç»“æœ?
å¯¹å®¢æˆ·ç«¯å®Œå…¨é€æ˜ï¼Œæ— éœ€å®¢æˆ·ç«¯åšä»»ä½•ä¿®æ”¹?
"""

import asyncio
import json
import time
import copy
from typing import AsyncGenerator, List, Optional, Union, Any

from loguru import logger

from geek_gateway.chunked_processor import ChunkedDocumentProcessor, CHARS_PER_TOKEN_ESTIMATE
from geek_gateway.config import settings, AUTO_CHUNK_THRESHOLD, CHUNK_MAX_CHARS, CHUNK_OVERLAP_CHARS


class AutoChunkedProcessor:
    """
    è‡ªåŠ¨åˆ†ç‰‡å¤„ç†å™?

    æ£€æµ‹é•¿æ–‡æ¡£å¹¶è‡ªåŠ¨åˆ†ç‰‡å¤„ç†ï¼Œå¯¹å®¢æˆ·ç«¯é€æ˜?
    """

    def __init__(
        self,
        threshold: int = None,
        max_chars: int = None,
        overlap_chars: int = None
    ):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åˆ†ç‰‡å¤„ç†å™¨?

        Args:
            threshold: è§¦å‘è‡ªåŠ¨åˆ†ç‰‡çš„é˜ˆå€¼ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½?
            max_chars: æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½?
            overlap_chars: åˆ†ç‰‡ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½?
        """
        self.threshold = threshold if threshold is not None else AUTO_CHUNK_THRESHOLD
        self.max_chars = max_chars if max_chars is not None else CHUNK_MAX_CHARS
        self.overlap_chars = overlap_chars if overlap_chars is not None else CHUNK_OVERLAP_CHARS
        self.processor = ChunkedDocumentProcessor(
            max_tokens_per_chunk=self.max_chars // CHARS_PER_TOKEN_ESTIMATE,
            overlap_tokens=self.overlap_chars // CHARS_PER_TOKEN_ESTIMATE
        )

    def extract_long_content(self, messages: List[Any]) -> tuple[Optional[str], int, str]:
        """
        ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–é•¿æ–‡æ¡£å†…å®?

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            (é•¿æ–‡æ¡£å†…? æ¶ˆæ¯ç´¢å¼•, å†…å®¹ç±»å‹) ?(None, -1, "")
            å†…å®¹ç±»å‹: "string" ã€?list"
        """
        for i, msg in enumerate(messages):
            # è·å– content
            if hasattr(msg, 'content'):
                content = msg.content
            elif isinstance(msg, dict):
                content = msg.get("content", "")
            else:
                continue

            # æ£€æŸ¥å­—ç¬¦ä¸²ç±»å‹
            if isinstance(content, str) and len(content) > self.threshold:
                return content, i, "string"

            # æ£€æŸ¥åˆ—è¡¨ç±»å‹ï¼ˆå¤šæ¨¡æ€å†…å®¹ï¼‰
            if isinstance(content, list):
                for block in content:
                    if isinstance(block, dict) and block.get("type") == "text":
                        text = block.get("text", "")
                        if len(text) > self.threshold:
                            return text, i, "list"

        return None, -1, ""

    def needs_chunking(self, messages: List[Any]) -> bool:
        """
        æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦éœ€è¦åˆ†ç‰‡å¤„ç?

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ˜¯å¦éœ€è¦åˆ†?
        """
        content, _, _ = self.extract_long_content(messages)
        return content is not None

    def create_chunked_messages(
        self,
        messages: List[Any],
        long_content: str,
        msg_index: int,
        content_type: str,
        chunk: str,
        chunk_index: int,
        total_chunks: int
    ) -> List[Any]:
        """
        åˆ›å»ºåŒ…å«åˆ†ç‰‡å†…å®¹çš„æ¶ˆæ¯åˆ—è¡?

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            long_content: åŸå§‹é•¿æ–‡æ¡£å†…?
            msg_index: é•¿æ–‡æ¡£æ‰€åœ¨çš„æ¶ˆæ¯ç´¢å¼•
            content_type: å†…å®¹ç±»å‹ ("string" ã€?list")
            chunk: å½“å‰åˆ†ç‰‡å†…å®¹
            chunk_index: å½“å‰åˆ†ç‰‡ç´¢å¼•
            total_chunks: æ€»åˆ†ç‰‡æ•°

        Returns:
            ä¿®æ”¹åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        # æ·±æ‹·è´æ¶ˆæ¯åˆ—?
        new_messages = copy.deepcopy(messages)

        # æ·»åŠ åˆ†ç‰‡ä¸Šä¸‹æ–‡ä¿¡?
        if total_chunks > 1:
            chunk_info = f"\n\n[è¿™æ˜¯é•¿æ–‡æ¡£çš„ã€‚{chunk_index + 1}/{total_chunks} éƒ¨åˆ†]"
            if chunk_index == 0:
                chunk_info += "\n[è¯·å¤„ç†è¿™éƒ¨åˆ†å†…å®¹ï¼Œåç»­ä¼šç»§ç»­æä¾›å‰©ä½™éƒ¨åˆ†]"
            elif chunk_index == total_chunks - 1:
                chunk_info += "\n[è¿™æ˜¯æœ€åä¸€éƒ¨åˆ†ï¼Œè¯·æ€»ç»“å®Œæˆå¤„ç†]"
            else:
                chunk_info += "\n[è¯·ç»§ç»­å¤„ç†è¿™éƒ¨åˆ†å†…å®¹]"

            chunk_with_info = chunk + chunk_info
        else:
            chunk_with_info = chunk

        # æ›¿æ¢æ¶ˆæ¯ä¸­çš„é•¿æ–‡æ¡£å†…?
        target_msg = new_messages[msg_index]

        if isinstance(target_msg, dict):
            if content_type == "string":
                target_msg["content"] = chunk_with_info
            else:  # list
                for block in target_msg.get("content", []):
                    if isinstance(block, dict) and block.get("type") == "text":
                        if block.get("text") == long_content:
                            block["text"] = chunk_with_info
                            break
        else:
            # Pydantic æ¨¡å‹
            if content_type == "string":
                target_msg.content = chunk_with_info
            else:  # list
                for block in target_msg.content:
                    if isinstance(block, dict) and block.get("type") == "text":
                        if block.get("text") == long_content:
                            block["text"] = chunk_with_info
                            break

        return new_messages

    def split_for_processing(self, long_content: str) -> List[str]:
        """
        å°†é•¿æ–‡æ¡£åˆ†å‰²æˆå¤šä¸ªåˆ†ç‰?

        Args:
            long_content: é•¿æ–‡æ¡£å†…?

        Returns:
            åˆ†ç‰‡åˆ—è¡¨
        """
        return self.processor.split_text(long_content)


# å…¨å±€å®ä¾‹
auto_chunked_processor = AutoChunkedProcessor()


async def process_with_auto_chunking(
    messages: List[Any],
    process_func,
    stream: bool = True,
    **kwargs
) -> AsyncGenerator[str, None]:
    """
    è‡ªåŠ¨åˆ†ç‰‡å¤„ç†é•¿æ–‡æ¡?

    å¦‚æœæ£€æµ‹åˆ°é•¿æ–‡æ¡£ï¼Œè‡ªåŠ¨åˆ†ç‰‡å¤„ç†å¹¶æµå¼è¿”å›ç»“æ?
    å¯¹å®¢æˆ·ç«¯å®Œå…¨é€æ˜?

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        process_func: å¤„ç†å•ä¸ªè¯·æ±‚çš„å¼‚æ­¥å‡½?
        stream: æ˜¯å¦æµå¼è¿”å›
        **kwargs: ä¼ é€’ç»™ process_func çš„å…¶ä»–å‚?

    Yields:
        SSE æ ¼å¼çš„å“åº”æ•°?
    """
    processor = auto_chunked_processor

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†?
    long_content, msg_index, content_type = processor.extract_long_content(messages)

    if long_content is None:
        # ä¸éœ€è¦åˆ†ç‰‡ï¼Œç›´æ¥å¤„ç†
        async for chunk in process_func(messages=messages, stream=stream, **kwargs):
            yield chunk
        return

    # éœ€è¦åˆ†ç‰‡å¤„?
    chunks = processor.split_for_processing(long_content)
    total_chunks = len(chunks)

    logger.info(f"Auto-chunking enabled: splitting into {total_chunks} chunks")

    # ç”¨äºæ”¶é›†æ‰€æœ‰åˆ†ç‰‡çš„å“åº”ï¼ˆéæµå¼æ¨¡å¼?
    all_responses = []

    for i, chunk in enumerate(chunks):
        logger.info(f"Processing chunk {i + 1}/{total_chunks} ({len(chunk)} chars)")

        # åˆ›å»ºåŒ…å«å½“å‰åˆ†ç‰‡çš„æ¶ˆ?
        chunked_messages = processor.create_chunked_messages(
            messages=messages,
            long_content=long_content,
            msg_index=msg_index,
            content_type=content_type,
            chunk=chunk,
            chunk_index=i,
            total_chunks=total_chunks
        )

        if stream:
            # æµå¼æ¨¡å¼ï¼šç›´æ¥è½¬å‘æ¯ä¸ªåˆ†ç‰‡çš„å“åº”
            if i > 0:
                # åœ¨åˆ†ç‰‡ä¹‹é—´æ·»åŠ åˆ†éš”ç¬¦
                separator = f"\n\n--- [ç»§ç»­å¤„ç†ã€‚{i + 1}/{total_chunks} éƒ¨åˆ†] ---\n\n"
                yield f"data: {json.dumps({'choices': [{'delta': {'content': separator}}]})}\n\n"

            async for response_chunk in process_func(messages=chunked_messages, stream=True, **kwargs):
                yield response_chunk
        else:
            # éæµå¼æ¨¡å¼ï¼šæ”¶é›†å“åº”ååˆ?
            response_content = ""
            async for response_chunk in process_func(messages=chunked_messages, stream=True, **kwargs):
                # è§£æå“åº”æå–å†…å®¹
                if response_chunk.startswith("data: "):
                    data_str = response_chunk[6:].strip()
                    if data_str and data_str != "[DONE]":
                        try:
                            data = json.loads(data_str)
                            delta = data.get("choices", [{}])[0].get("delta", {})
                            if "content" in delta:
                                response_content += delta["content"]
                        except json.JSONDecodeError:
                            pass

            all_responses.append(response_content)

    if not stream and all_responses:
        # éæµå¼æ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰å“åº”å¹¶è¿”å›
        merged_content = "\n\n".join(all_responses)
        final_response = {
            "id": f"chatcmpl-chunked-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": kwargs.get("model", "unknown"),
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": merged_content
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
        yield json.dumps(final_response)

    logger.info(f"Auto-chunking completed: processed {total_chunks} chunks")
