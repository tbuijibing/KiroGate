# -*- coding: utf-8 -*-

# GeekGate
# Based on kiro-openai-gateway by Jwadow (https://github.com/Jwadow/kiro-openai-gateway)
# Original Copyright (C) 2025 Jwadow
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""
Streaming response processing logic, converts Kiro stream to OpenAI/Anthropic format.

Contains generators for:
- Converting AWS SSE to OpenAI SSE
- Forming streaming chunks
- Processing tool calls in stream
- Adaptive timeout handling for slow models
"""

import asyncio
import json
import time
from typing import TYPE_CHECKING, AsyncGenerator, Callable, Awaitable, Optional, Dict, Any, List

import httpx
from fastapi import HTTPException
from loguru import logger

from geek_gateway.parsers import AwsEventStreamParser, parse_bracket_tool_calls, deduplicate_tool_calls
from geek_gateway.utils import generate_completion_id
from geek_gateway.config import settings, get_adaptive_timeout
from geek_gateway.tokenizer import count_tokens, count_message_tokens, count_tools_tokens
from geek_gateway.thinking_parser import KiroThinkingTagParser, SegmentType, TextSegment

if TYPE_CHECKING:
    from geek_gateway.auth import GeekAuthManager
    from geek_gateway.cache import ModelInfoCache

try:
    from geek_gateway.debug_logger import debug_logger
except ImportError:
    debug_logger = None


class FirstTokenTimeoutError(Exception):
    """Exception raised when first token timeout occurs."""
    pass


class StreamReadTimeoutError(Exception):
    """Exception raised when stream read timeout occurs."""
    pass


async def _read_chunk_with_timeout(
    byte_iterator,
    timeout: float
) -> bytes:
    """
    Read a chunk from byte iterator with timeout.

    Args:
        byte_iterator: Async byte iterator
        timeout: Timeout in seconds

    Returns:
        Bytes chunk

    Raises:
        StreamReadTimeoutError: If timeout occurs
        StopAsyncIteration: If iterator is exhausted
    """
    try:
        return await asyncio.wait_for(
            byte_iterator.__anext__(),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        raise StreamReadTimeoutError(f"æµå¼è¯»å–åœ?{timeout}s åè¶…æ—?)


def _calculate_usage_tokens(
    full_content: str,
    context_usage_percentage: Optional[float],
    model_cache: "ModelInfoCache",
    model: str,
    request_messages: Optional[list],
    request_tools: Optional[list]
) -> Dict[str, Any]:
    """
    Calculate token usage from response.

    Args:
        full_content: Full response content
        context_usage_percentage: Context usage percentage from API
        model_cache: Model cache for token limits
        model: Model name
        request_messages: Request messages for fallback counting
        request_tools: Request tools for fallback counting

    Returns:
        Dict with prompt_tokens, completion_tokens, total_tokens and source info
    """
    completion_tokens = count_tokens(full_content)

    total_tokens_from_api = 0
    if context_usage_percentage is not None and context_usage_percentage > 0:
        max_input_tokens = model_cache.get_max_input_tokens(model)
        total_tokens_from_api = int((context_usage_percentage / 100) * max_input_tokens)

    if total_tokens_from_api > 0:
        prompt_tokens = max(0, total_tokens_from_api - completion_tokens)
        total_tokens = total_tokens_from_api
        prompt_source = "subtraction"
        total_source = "API Kiro"
    else:
        prompt_tokens = 0
        if request_messages:
            prompt_tokens += count_message_tokens(request_messages, apply_claude_correction=False)
        if request_tools:
            prompt_tokens += count_tools_tokens(request_tools, apply_claude_correction=False)
        total_tokens = prompt_tokens + completion_tokens
        prompt_source = "tiktoken"
        total_source = "tiktoken"

    return {
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "prompt_source": prompt_source,
        "total_source": total_source
    }


def _format_tool_calls_for_streaming(tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Format tool calls for streaming response with required index field.

    Args:
        tool_calls: List of tool calls

    Returns:
        List of indexed tool calls for streaming
    """
    indexed_tool_calls = []
    for idx, tc in enumerate(tool_calls):
        func = tc.get("function") or {}
        tool_name = func.get("name") or ""
        tool_args = func.get("arguments") or "{}"

        logger.debug(f"Tool call [{idx}] '{tool_name}': id={tc.get('id')}, args_length={len(tool_args)}")

        indexed_tc = {
            "index": idx,
            "id": tc.get("id"),
            "type": tc.get("type", "function"),
            "function": {
                "name": tool_name,
                "arguments": tool_args
            }
        }
        indexed_tool_calls.append(indexed_tc)

    return indexed_tool_calls


def _format_tool_calls_for_non_streaming(tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Format tool calls for non-streaming response (without index field).

    Args:
        tool_calls: List of tool calls

    Returns:
        List of cleaned tool calls for non-streaming
    """
    cleaned_tool_calls = []
    for tc in tool_calls:
        func = tc.get("function") or {}
        cleaned_tc = {
            "id": tc.get("id"),
            "type": tc.get("type", "function"),
            "function": {
                "name": func.get("name", ""),
                "arguments": func.get("arguments", "{}")
            }
        }
        cleaned_tool_calls.append(cleaned_tc)

    return cleaned_tool_calls


async def stream_kiro_to_openai_internal(
    client: httpx.AsyncClient,
    response: httpx.Response,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    first_token_timeout: float = settings.first_token_timeout,
    stream_read_timeout: float = settings.stream_read_timeout,
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None
) -> AsyncGenerator[str, None]:
    """
    Internal generator for converting Kiro stream to OpenAI format.

    Parses AWS SSE stream and converts events to OpenAI chat.completion.chunk.
    Supports tool calls and usage calculation.

    IMPORTANT: This function raises FirstTokenTimeoutError if first token
    is not received within first_token_timeout seconds.

    Args:
        client: HTTP client (for connection management)
        response: HTTP response with data stream
        model: Model name to include in response
        model_cache: Model cache for token limits
        auth_manager: Authentication manager
        first_token_timeout: First token timeout (seconds)
        stream_read_timeout: Stream read timeout for subsequent chunks (seconds)
        request_messages: Original request messages (for fallback token counting)
        request_tools: Original request tools (for fallback token counting)

    Yields:
        Strings in SSE format: "data: {...}\\n\\n" or "data: [DONE]\\n\\n"

    Raises:
        FirstTokenTimeoutError: If first token not received within timeout
        StreamReadTimeoutError: If stream read times out
    """
    completion_id = generate_completion_id()
    created_time = int(time.time())
    first_chunk = True

    parser = AwsEventStreamParser()
    metering_data = None
    context_usage_percentage = None
    content_parts: list[str] = []  # ä½¿ç”¨ list æ›¿ä»£å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œæå‡æ€§èƒ½

    # æ ¹æ®æ¨¡å‹è‡ªé€‚åº”è°ƒæ•´è¶…æ—¶æ—¶é—´
    adaptive_first_token_timeout = get_adaptive_timeout(model, first_token_timeout)
    adaptive_stream_read_timeout = get_adaptive_timeout(model, stream_read_timeout)

    try:
        byte_iterator = response.aiter_bytes()

        # Wait for first chunk with adaptive timeout
        try:
            first_byte_chunk = await asyncio.wait_for(
                byte_iterator.__anext__(),
                timeout=adaptive_first_token_timeout
            )
        except asyncio.TimeoutError:
            logger.warning(f"First token timeout after {adaptive_first_token_timeout}s (model: {model})")
            raise FirstTokenTimeoutError(f"ã€‚{adaptive_first_token_timeout}s å†…æœªæ”¶åˆ°å“åº”")
        except StopAsyncIteration:
            logger.debug("Empty response from Kiro API")
            yield "data: [DONE]\n\n"
            return

        # Process first chunk
        if debug_logger:
            debug_logger.log_raw_chunk(first_byte_chunk)

        events = parser.feed(first_byte_chunk)
        for event in events:
            if event["type"] == "content":
                content = event["data"]
                content_parts.append(content)

                delta = {"content": content}
                if first_chunk:
                    delta["role"] = "assistant"
                    first_chunk = False

                openai_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": model,
                    "choices": [{"index": 0, "delta": delta, "finish_reason": None}]
                }

                chunk_text = f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

                if debug_logger:
                    debug_logger.log_modified_chunk(chunk_text.encode('utf-8'))

                yield chunk_text

            elif event["type"] == "usage":
                metering_data = event["data"]

            elif event["type"] == "context_usage":
                context_usage_percentage = event["data"]

        # Continue reading remaining chunks with adaptive timeout
        # å¯¹äºæ…¢æ¨¡å‹å’Œå¤§æ–‡æ¡£ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ç­‰å¾…æ¯?chunk
        consecutive_timeouts = 0
        max_consecutive_timeouts = 3  # å…è®¸è¿ç»­è¶…æ—¶æ¬¡æ•°
        while True:
            try:
                chunk = await _read_chunk_with_timeout(byte_iterator, adaptive_stream_read_timeout)
                consecutive_timeouts = 0  # é‡ç½®è¶…æ—¶è®¡æ•°?
            except StopAsyncIteration:
                break
            except StreamReadTimeoutError as e:
                consecutive_timeouts += 1
                if consecutive_timeouts <= max_consecutive_timeouts:
                    logger.warning(
                        f"Stream read timeout {consecutive_timeouts}/{max_consecutive_timeouts} "
                        f"after {adaptive_stream_read_timeout}s (model: {model}). "
                        f"Model may be processing large content - continuing to wait..."
                    )
                    # ç»§ç»­ç­‰å¾…ä¸‹ä¸€?chunk
                    continue
                else:
                    logger.error(f"Stream read timeout after {max_consecutive_timeouts} consecutive timeouts (model: {model}): {e}")
                    raise

            if debug_logger:
                debug_logger.log_raw_chunk(chunk)

            events = parser.feed(chunk)

            for event in events:
                if event["type"] == "content":
                    content = event["data"]
                    content_parts.append(content)

                    delta = {"content": content}
                    if first_chunk:
                        delta["role"] = "assistant"
                        first_chunk = False

                    openai_chunk = {
                        "id": completion_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{"index": 0, "delta": delta, "finish_reason": None}]
                    }

                    chunk_text = f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

                    if debug_logger:
                        debug_logger.log_modified_chunk(chunk_text.encode('utf-8'))

                    yield chunk_text

                elif event["type"] == "usage":
                    metering_data = event["data"]

                elif event["type"] == "context_usage":
                    context_usage_percentage = event["data"]

        # åˆå¹¶ content éƒ¨åˆ†ï¼ˆæ¯”å­—ç¬¦ä¸²æ‹¼æ¥æ›´é«˜æ•ˆ?
        full_content = ''.join(content_parts)

        # Check bracket-style tool calls in full content
        bracket_tool_calls = parse_bracket_tool_calls(full_content)
        all_tool_calls = parser.get_tool_calls() + bracket_tool_calls
        all_tool_calls = deduplicate_tool_calls(all_tool_calls)

        finish_reason = "tool_calls" if all_tool_calls else "stop"

        # Calculate usage tokens using helper function
        usage_info = _calculate_usage_tokens(
            full_content, context_usage_percentage, model_cache, model,
            request_messages, request_tools
        )

        # Send tool calls if any
        if all_tool_calls:
            logger.debug(f"Processing {len(all_tool_calls)} tool calls for streaming response")
            indexed_tool_calls = _format_tool_calls_for_streaming(all_tool_calls)

            tool_calls_chunk = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_time,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"tool_calls": indexed_tool_calls},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(tool_calls_chunk, ensure_ascii=False)}\n\n"

        # Final chunk with usage
        final_chunk = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created_time,
            "model": model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}],
            "usage": {
                "prompt_tokens": usage_info["prompt_tokens"],
                "completion_tokens": usage_info["completion_tokens"],
                "total_tokens": usage_info["total_tokens"],
            }
        }

        if metering_data:
            final_chunk["usage"]["credits_used"] = metering_data

        logger.debug(
            f"[Usage] {model}: "
            f"prompt_tokens={usage_info['prompt_tokens']} ({usage_info['prompt_source']}), "
            f"completion_tokens={usage_info['completion_tokens']} (tiktoken), "
            f"total_tokens={usage_info['total_tokens']} ({usage_info['total_source']})"
        )

        yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

    except FirstTokenTimeoutError:
        raise
    except StreamReadTimeoutError:
        raise
    except Exception as e:
        logger.error(f"Error during streaming: {e}", exc_info=True)
    finally:
        await response.aclose()
        logger.debug("Streaming completed")


async def stream_kiro_to_openai(
    client: httpx.AsyncClient,
    response: httpx.Response,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None
) -> AsyncGenerator[str, None]:
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Kiro Ğ² OpenAI Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚.
    
    Ğ­Ñ‚Ğ¾ wrapper Ğ½Ğ°Ğ´ stream_kiro_to_openai_internal, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞĞ• Ğ´ĞµĞ»Ğ°ĞµÑ‚ retry.
    Retry Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² stream_with_first_token_retry.
    
    Args:
        client: HTTP ĞºĞ»Ğ¸ĞµĞ½Ñ‚ (Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼)
        response: HTTP Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚
        model_cache: ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
        auth_manager: ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        request_messages: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
        request_tools: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
    
    Yields:
        Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ SSE: "data: {...}\\n\\n" Ğ¸Ğ»Ğ¸ "data: [DONE]\\n\\n"
    """
    async for chunk in stream_kiro_to_openai_internal(
        client, response, model, model_cache, auth_manager,
        request_messages=request_messages,
        request_tools=request_tools
    ):
        yield chunk


async def stream_with_first_token_retry(
    make_request: Callable[[], Awaitable[httpx.Response]],
    client: httpx.AsyncClient,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    max_retries: int = settings.first_token_max_retries,
    first_token_timeout: float = settings.first_token_timeout,
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None
) -> AsyncGenerator[str, None]:
    """
    Streaming Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ retry Ğ¿Ñ€Ğ¸ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°.
    
    Ğ•ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ first_token_timeout ÑĞµĞºÑƒĞ½Ğ´,
    Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹. ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ max_retries Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº.
    
    Ğ­Ñ‚Ğ¾ seamless Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ - Ğ¾Ğ½ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ,
    Ğ½Ğ¾ Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ (Ğ¸Ğ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº).
    
    Args:
        make_request: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ HTTP Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        client: HTTP ĞºĞ»Ğ¸ĞµĞ½Ñ‚
        model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model_cache: ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        auth_manager: ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        max_retries: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº
        first_token_timeout: Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (ÑĞµĞºÑƒĞ½Ğ´Ñ‹)
        request_messages: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
        request_tools: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
    
    Yields:
        Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ SSE
    
    Raises:
        HTTPException: ĞŸĞ¾ÑĞ»Ğµ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº
    
    Example:
        >>> async def make_req():
        ...     return await http_client.request_with_retry("POST", url, payload, stream=True)
        >>> async for chunk in stream_with_first_token_retry(make_req, client, model, cache, auth):
        ...     print(chunk)
    """
    last_error: Optional[Exception] = None
    
    for attempt in range(max_retries):
        response: Optional[httpx.Response] = None
        try:
            # Ğ”ĞµĞ»Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
            if attempt > 0:
                logger.warning(f"Retry attempt {attempt + 1}/{max_retries} after first token timeout")
            
            response = await make_request()
            
            if response.status_code != 200:
                # ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚ API - Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ response Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ
                try:
                    error_content = await response.aread()
                    error_text = error_content.decode('utf-8', errors='replace')
                except Exception:
                    error_text = "æœªçŸ¥é”™è¯¯"
                
                try:
                    await response.aclose()
                except Exception:
                    pass
                
                logger.error(f"Error from Kiro API: {response.status_code} - {error_text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"ä¸Šæ¸¸ API é”™è¯¯: {error_text}"
                )
            
            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ñ‚ÑŒ Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½
            async for chunk in stream_kiro_to_openai_internal(
                client,
                response,
                model,
                model_cache,
                auth_manager,
                first_token_timeout=first_token_timeout,
                request_messages=request_messages,
                request_tools=request_tools
            ):
                yield chunk
            
            # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¸ - Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ğ¼
            return
            
        except FirstTokenTimeoutError as e:
            last_error = e
            logger.warning(f"First token timeout on attempt {attempt + 1}/{max_retries}")
            
            # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ response ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚
            if response:
                try:
                    await response.aclose()
                except Exception:
                    pass
            
            # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞµ
            continue
            
        except Exception as e:
            # Ğ”Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ - Ğ½Ğµ retry, Ğ¿Ñ€Ğ¾Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼
            logger.error(f"Unexpected error during streaming: {e}", exc_info=True)
            if response:
                try:
                    await response.aclose()
                except Exception:
                    pass
            raise
    
    # Ğ’ÑĞµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ñ‹ - Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ HTTP Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ
    logger.error(f"All {max_retries} attempts failed due to first token timeout")
    raise HTTPException(
        status_code=504,
        detail=f"æ¨¡å‹åœ?{max_retries} æ¬¡å°è¯•åä»æœªåœ?{first_token_timeout}s å†…å“åº”ï¼Œè¯·ç¨åå†è¯?
    )


async def collect_stream_response(
    client: httpx.AsyncClient,
    response: httpx.Response,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None
) -> dict:
    """
    Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· streaming Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°.
    
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ non-streaming Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° - ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²ÑĞµ chunks
    Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚.
    
    Args:
        client: HTTP ĞºĞ»Ğ¸ĞµĞ½Ñ‚
        response: HTTP Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼
        model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model_cache: ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        auth_manager: ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        request_messages: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
        request_tools: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ fallback Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
    
    Returns:
        Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ OpenAI chat.completion
    """
    content_parts: list[str] = []  # ä½¿ç”¨ list æ›¿ä»£å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œæå‡æ€§èƒ½
    final_usage = None
    tool_calls = []
    completion_id = generate_completion_id()

    async for chunk_str in stream_kiro_to_openai(
        client,
        response,
        model,
        model_cache,
        auth_manager,
        request_messages=request_messages,
        request_tools=request_tools
    ):
        if not chunk_str.startswith("data:"):
            continue
        
        data_str = chunk_str[len("data:"):].strip()
        if not data_str or data_str == "[DONE]":
            continue
        
        try:
            chunk_data = json.loads(data_str)
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· chunk
            delta = chunk_data.get("choices", [{}])[0].get("delta", {})
            if "content" in delta:
                content_parts.append(delta["content"])
            if "tool_calls" in delta:
                tool_calls.extend(delta["tool_calls"])
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ usage Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ chunk
            if "usage" in chunk_data:
                final_usage = chunk_data["usage"]
                
        except (json.JSONDecodeError, IndexError):
            continue

    # åˆå¹¶ content éƒ¨åˆ†ï¼ˆæ¯”å­—ç¬¦ä¸²æ‹¼æ¥æ›´é«˜æ•ˆ?
    full_content = ''.join(content_parts)

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
    message = {"role": "assistant", "content": full_content}
    if tool_calls:
        # Ğ”Ğ»Ñ non-streaming Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ index Ğ¸Ğ· tool_calls,
        # Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ streaming chunks
        cleaned_tool_calls = []
        for tc in tool_calls:
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ function Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ¾Ñ‚ None
            func = tc.get("function") or {}
            cleaned_tc = {
                "id": tc.get("id"),
                "type": tc.get("type", "function"),
                "function": {
                    "name": func.get("name", ""),
                    "arguments": func.get("arguments", "{}")
                }
            }
            cleaned_tool_calls.append(cleaned_tc)
        message["tool_calls"] = cleaned_tool_calls
    
    finish_reason = "tool_calls" if tool_calls else "stop"
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ usage Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
    usage = final_usage or {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    
    # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ (non-streaming Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğµ Ğ¶Ğµ Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ· streaming)
    
    return {
        "id": completion_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "message": message,
            "finish_reason": finish_reason
        }],
        "usage": usage
    }


# ==================================================================================================
# Anthropic Streaming Functions
# ==================================================================================================

def generate_anthropic_message_id() -> str:
    """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ID ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Anthropic."""
    import uuid
    return f"msg_{uuid.uuid4().hex[:24]}"


async def stream_kiro_to_anthropic(
    client: httpx.AsyncClient,
    response: httpx.Response,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None,
    thinking_enabled: bool = False,
    stream_read_timeout: float = settings.stream_read_timeout
) -> AsyncGenerator[str, None]:
    """
    ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Kiro Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Anthropic SSE.

    Anthropic Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹:
    - message_start: Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
    - content_block_start: Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
    - content_block_delta: Ğ´ĞµĞ»ÑŒÑ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (text_delta, thinking_delta Ğ¸Ğ»Ğ¸ input_json_delta)
    - content_block_stop: ĞºĞ¾Ğ½ĞµÑ† Ğ±Ğ»Ğ¾ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
    - message_delta: Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ (stop_reason, usage)
    - message_stop: ĞºĞ¾Ğ½ĞµÑ† ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ

    ?thinking_enabled=True æ—¶ï¼Œä¼šè§£?Kiro è¿”å›?<thinking>...</thinking> æ ‡ç­¾?
    å¹¶è½¬æ¢ä¸º Anthropic å®˜æ–¹?thinking_delta äº‹ä»¶æ ¼å¼?

    Args:
        client: HTTP ĞºĞ»Ğ¸ĞµĞ½Ñ‚
        response: HTTP Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼
        model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model_cache: ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        auth_manager: ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        request_messages: Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
        request_tools: Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
        thinking_enabled: Ğ’ĞºĞ»ÑÑ‡ĞµĞ½ Ğ»Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ thinking
        stream_read_timeout: Stream read timeout for each chunk (seconds)

    Yields:
        Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Anthropic SSE
    """
    message_id = generate_anthropic_message_id()
    parser = AwsEventStreamParser()
    metering_data = None
    context_usage_percentage = None
    content_parts: list[str] = []  # ç”¨äº token è®¡ç®—çš„å®Œæ•´å†…?
    thinking_parts: list[str] = []  # thinking å†…å®¹ï¼ˆç”¨?token è®¡ç®—?
    text_parts: list[str] = []  # æ™®é€šæ–‡æœ¬å†…å®¹ï¼ˆç”¨äº token è®¡ç®—?
    content_block_index = 0
    thinking_block_started = False
    text_block_started = False

    # Thinking è§£æå™¨ï¼ˆä»…åœ¨ thinking_enabled æ—¶ä½¿ç”¨ï¼‰
    thinking_parser = KiroThinkingTagParser() if thinking_enabled else None

    # æ ¹æ®æ¨¡å‹è‡ªé€‚åº”è°ƒæ•´è¶…æ—¶æ—¶é—´
    adaptive_stream_read_timeout = get_adaptive_timeout(model, stream_read_timeout)

    # Pre-calculate input_tokens (can be determined before stream starts)
    # This ensures message_start event contains real input_tokens value
    pre_calculated_input_tokens = 0
    if request_messages:
        pre_calculated_input_tokens += count_message_tokens(request_messages, apply_claude_correction=False)
    if request_tools:
        pre_calculated_input_tokens += count_tools_tokens(request_tools, apply_claude_correction=False)

    async def emit_thinking_segment(content: str) -> AsyncGenerator[str, None]:
        """å?thinking å†…å®¹çš„äº‹ã€?""
        nonlocal content_block_index, thinking_block_started, thinking_parts

        if not content:
            return

        thinking_parts.append(content)

        # å¦‚æœ thinking block è¿˜æ²¡å¼€å§‹ï¼Œå…ˆå‘?content_block_start
        if not thinking_block_started:
            block_start = {
                "type": "content_block_start",
                "index": content_block_index,
                "content_block": {"type": "thinking", "thinking": ""}
            }
            yield f"event: content_block_start\ndata: {json.dumps(block_start, ensure_ascii=False)}\n\n"
            thinking_block_started = True

        # å?thinking_delta
        delta = {
            "type": "content_block_delta",
            "index": content_block_index,
            "delta": {"type": "thinking_delta", "thinking": content}
        }
        yield f"event: content_block_delta\ndata: {json.dumps(delta, ensure_ascii=False)}\n\n"

        if debug_logger:
            debug_logger.log_modified_chunk(f"event: content_block_delta\ndata: {json.dumps(delta)}\n\n".encode('utf-8'))

    async def close_thinking_block() -> AsyncGenerator[str, None]:
        """å…³é—­ thinking block"""
        nonlocal content_block_index, thinking_block_started

        if thinking_block_started:
            block_stop = {
                "type": "content_block_stop",
                "index": content_block_index
            }
            yield f"event: content_block_stop\ndata: {json.dumps(block_stop, ensure_ascii=False)}\n\n"
            content_block_index += 1
            thinking_block_started = False

    async def emit_text_segment(content: str) -> AsyncGenerator[str, None]:
        """å‘é€æ™®é€šæ–‡æœ¬å†…å®¹çš„äº‹ä»¶"""
        nonlocal content_block_index, text_block_started, text_parts

        if not content:
            return

        text_parts.append(content)

        # å¦‚æœ text block è¿˜æ²¡å¼€å§‹ï¼Œå…ˆå‘?content_block_start
        if not text_block_started:
            block_start = {
                "type": "content_block_start",
                "index": content_block_index,
                "content_block": {"type": "text", "text": ""}
            }
            yield f"event: content_block_start\ndata: {json.dumps(block_start, ensure_ascii=False)}\n\n"
            text_block_started = True

        # å?text_delta
        delta = {
            "type": "content_block_delta",
            "index": content_block_index,
            "delta": {"type": "text_delta", "text": content}
        }
        yield f"event: content_block_delta\ndata: {json.dumps(delta, ensure_ascii=False)}\n\n"

        if debug_logger:
            debug_logger.log_modified_chunk(f"event: content_block_delta\ndata: {json.dumps(delta)}\n\n".encode('utf-8'))

    async def close_text_block() -> AsyncGenerator[str, None]:
        """å…³é—­ text block"""
        nonlocal content_block_index, text_block_started

        if text_block_started:
            block_stop = {
                "type": "content_block_stop",
                "index": content_block_index
            }
            yield f"event: content_block_stop\ndata: {json.dumps(block_stop, ensure_ascii=False)}\n\n"
            content_block_index += 1
            text_block_started = False

    try:
        # message_start
        message_start = {
            "type": "message_start",
            "message": {
                "id": message_id,
                "type": "message",
                "role": "assistant",
                "content": [],
                "model": model,
                "stop_reason": None,
                "stop_sequence": None,
                "usage": {
                    "input_tokens": pre_calculated_input_tokens,
                    "output_tokens": 0,
                    "cache_creation_input_tokens": 0,
                    "cache_read_input_tokens": 0
                }
            }
        }

        yield f"event: message_start\ndata: {json.dumps(message_start, ensure_ascii=False)}\n\n"

        # Read chunks with adaptive timeout
        byte_iterator = response.aiter_bytes()
        consecutive_timeouts = 0
        max_consecutive_timeouts = 3

        while True:
            try:
                chunk = await _read_chunk_with_timeout(byte_iterator, adaptive_stream_read_timeout)
                consecutive_timeouts = 0
            except StopAsyncIteration:
                break
            except StreamReadTimeoutError as e:
                consecutive_timeouts += 1
                if consecutive_timeouts <= max_consecutive_timeouts:
                    logger.warning(
                        f"Anthropic stream timeout {consecutive_timeouts}/{max_consecutive_timeouts} "
                        f"after {adaptive_stream_read_timeout}s (model: {model}). "
                        f"Model may be processing large content - continuing to wait..."
                    )
                    continue
                else:
                    logger.error(f"Anthropic stream read timeout after {max_consecutive_timeouts} consecutive timeouts (model: {model}): {e}")
                    raise

            if debug_logger:
                debug_logger.log_raw_chunk(chunk)

            events = parser.feed(chunk)

            for event in events:
                if event["type"] == "content":
                    content = event["data"]
                    content_parts.append(content)

                    if thinking_enabled and thinking_parser:
                        # ä½¿ç”¨ thinking è§£æå™¨å¤„ç†å†…?
                        segments = thinking_parser.push_and_parse(content)

                        for segment in segments:
                            if segment.type == SegmentType.THINKING:
                                # å¦‚æœä¹‹å‰?text block æ‰“å¼€ï¼Œå…ˆå…³é—­?
                                async for event_str in close_text_block():
                                    yield event_str
                                # å?thinking å†…å®¹
                                async for event_str in emit_thinking_segment(segment.content):
                                    yield event_str
                            elif segment.type == SegmentType.TEXT:
                                # å¦‚æœä¹‹å‰?thinking block æ‰“å¼€ï¼Œå…ˆå…³é—­?
                                async for event_str in close_thinking_block():
                                    yield event_str
                                # å‘é€æ™®é€šæ–‡?
                                async for event_str in emit_text_segment(segment.content):
                                    yield event_str
                    else:
                        # ä¸å¯?thinking è§£æï¼Œç›´æ¥ä½œä¸ºæ–‡æœ¬å¤„?
                        async for event_str in emit_text_segment(content):
                            yield event_str

                elif event["type"] == "usage":
                    metering_data = event["data"]

                elif event["type"] == "context_usage":
                    context_usage_percentage = event["data"]

        # æµç»“æŸï¼Œåˆ·æ–° thinking è§£æå™¨ç¼“å†²åŒº
        if thinking_enabled and thinking_parser:
            final_segments = thinking_parser.flush()
            for segment in final_segments:
                if segment.type == SegmentType.THINKING:
                    async for event_str in close_text_block():
                        yield event_str
                    async for event_str in emit_thinking_segment(segment.content):
                        yield event_str
                elif segment.type == SegmentType.TEXT:
                    async for event_str in close_thinking_block():
                        yield event_str
                    async for event_str in emit_text_segment(segment.content):
                        yield event_str

        # å…³é—­æ‰€æœ‰æ‰“å¼€?blocks
        async for event_str in close_thinking_block():
            yield event_str
        async for event_str in close_text_block():
            yield event_str

        # åˆå¹¶ content éƒ¨åˆ†ï¼ˆç”¨?token è®¡ç®—?
        full_content = ''.join(content_parts)

        # å¤„ç† tool calls
        bracket_tool_calls = parse_bracket_tool_calls(full_content)
        all_tool_calls = parser.get_tool_calls() + bracket_tool_calls
        all_tool_calls = deduplicate_tool_calls(all_tool_calls)

        # å?tool_use blocks
        for tc in all_tool_calls:
            func = tc.get("function") or {}
            tool_name = func.get("name") or ""
            tool_args_str = func.get("arguments") or "{}"
            tool_id = tc.get("id") or f"toolu_{generate_completion_id()[8:]}"

            try:
                tool_input = json.loads(tool_args_str)
            except json.JSONDecodeError:
                tool_input = {}

            # content_block_start for tool_use
            tool_block_start = {
                "type": "content_block_start",
                "index": content_block_index,
                "content_block": {
                    "type": "tool_use",
                    "id": tool_id,
                    "name": tool_name,
                    "input": {}
                }
            }
            yield f"event: content_block_start\ndata: {json.dumps(tool_block_start, ensure_ascii=False)}\n\n"

            # input_json_delta
            if tool_input:
                input_delta = {
                    "type": "content_block_delta",
                    "index": content_block_index,
                    "delta": {
                        "type": "input_json_delta",
                        "partial_json": json.dumps(tool_input, ensure_ascii=False)
                    }
                }
                yield f"event: content_block_delta\ndata: {json.dumps(input_delta, ensure_ascii=False)}\n\n"

            # content_block_stop
            tool_block_stop = {
                "type": "content_block_stop",
                "index": content_block_index
            }
            yield f"event: content_block_stop\ndata: {json.dumps(tool_block_stop, ensure_ascii=False)}\n\n"

            content_block_index += 1

        # ç¡®å®š stop_reason
        stop_reason = "tool_use" if all_tool_calls else "end_turn"

        # è®¡ç®— token ä½¿ç”¨?
        usage_info = _calculate_usage_tokens(
            full_content, context_usage_percentage, model_cache, model,
            request_messages, request_tools
        )
        input_tokens = usage_info["prompt_tokens"]
        completion_tokens = usage_info["completion_tokens"]

        # å?message_delta
        message_delta = {
            "type": "message_delta",
            "delta": {
                "stop_reason": stop_reason,
                "stop_sequence": None
            },
            "usage": {
                "output_tokens": completion_tokens
            }
        }
        yield f"event: message_delta\ndata: {json.dumps(message_delta, ensure_ascii=False)}\n\n"

        # å?message_stop
        yield f"event: message_stop\ndata: {{\"type\": \"message_stop\"}}\n\n"

        if thinking_enabled and thinking_parser and thinking_parser.has_extracted_thinking:
            logger.debug(
                f"[Anthropic Usage with Thinking] {model}: input_tokens={input_tokens}, "
                f"output_tokens={completion_tokens}, thinking_chars={len(''.join(thinking_parts))}"
            )
        else:
            logger.debug(
                f"[Anthropic Usage] {model}: input_tokens={input_tokens}, output_tokens={completion_tokens}"
            )

    except Exception as e:
        # ç¡®ä¿é”™è¯¯ä¿¡æ¯ä¸ä¸º?
        error_msg = str(e) if str(e) else f"{type(e).__name__}: {repr(e)}"
        logger.error(f"Error during Anthropic streaming: {error_msg}", exc_info=True)
        # å?error event
        error_event = {
            "type": "error",
            "error": {
                "type": "api_error",
                "message": error_msg
            }
        }
        yield f"event: error\ndata: {json.dumps(error_event, ensure_ascii=False)}\n\n"
    finally:
        await response.aclose()
        logger.debug("Anthropic streaming completed")


async def collect_anthropic_response(
    client: httpx.AsyncClient,
    response: httpx.Response,
    model: str,
    model_cache: "ModelInfoCache",
    auth_manager: "GeekAuthManager",
    request_messages: Optional[list] = None,
    request_tools: Optional[list] = None,
    thinking_enabled: bool = False,
    stream_read_timeout: float = settings.stream_read_timeout
) -> dict:
    """
    Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· streaming Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Anthropic.

    ?thinking_enabled=True æ—¶ï¼Œä¼šè§£?Kiro è¿”å›?<thinking>...</thinking> æ ‡ç­¾?
    å¹¶åœ¨å“åº”ä¸­æ·»?thinking content block?

    Args:
        client: HTTP ĞºĞ»Ğ¸ĞµĞ½Ñ‚
        response: HTTP Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼
        model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model_cache: ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        auth_manager: ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        request_messages: Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        request_tools: Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        thinking_enabled: Ğ’ĞºĞ»ÑÑ‡ĞµĞ½ Ğ»Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ thinking
        stream_read_timeout: Stream read timeout for each chunk (seconds)

    Returns:
        Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Anthropic Messages API
    """
    message_id = generate_anthropic_message_id()
    parser = AwsEventStreamParser()
    metering_data = None
    context_usage_percentage = None
    content_parts: list[str] = []

    # Thinking è§£æå™¨ï¼ˆä»…åœ¨ thinking_enabled æ—¶ä½¿ç”¨ï¼‰
    thinking_parser = KiroThinkingTagParser() if thinking_enabled else None

    # æ ¹æ®æ¨¡å‹è‡ªé€‚åº”è°ƒæ•´è¶…æ—¶æ—¶é—´
    adaptive_stream_read_timeout = get_adaptive_timeout(model, stream_read_timeout)

    try:
        # Read chunks with adaptive timeout
        byte_iterator = response.aiter_bytes()
        consecutive_timeouts = 0
        max_consecutive_timeouts = 3
        while True:
            try:
                chunk = await _read_chunk_with_timeout(byte_iterator, adaptive_stream_read_timeout)
                consecutive_timeouts = 0
            except StopAsyncIteration:
                break
            except StreamReadTimeoutError as e:
                consecutive_timeouts += 1
                if consecutive_timeouts <= max_consecutive_timeouts:
                    logger.warning(
                        f"Anthropic collect timeout {consecutive_timeouts}/{max_consecutive_timeouts} "
                        f"after {adaptive_stream_read_timeout}s (model: {model}). "
                        f"Model may be processing large content - continuing to wait..."
                    )
                    continue
                else:
                    logger.error(f"Anthropic collect stream read timeout after {max_consecutive_timeouts} consecutive timeouts (model: {model}): {e}")
                    raise

            if debug_logger:
                debug_logger.log_raw_chunk(chunk)

            events = parser.feed(chunk)

            for event in events:
                if event["type"] == "content":
                    content_parts.append(event["data"])
                elif event["type"] == "usage":
                    metering_data = event["data"]
                elif event["type"] == "context_usage":
                    context_usage_percentage = event["data"]

    finally:
        await response.aclose()

    # åˆå¹¶ content éƒ¨åˆ†
    full_content = ''.join(content_parts)

    # å¤„ç† thinking å†…å®¹
    thinking_content = ""
    text_content = full_content

    if thinking_enabled and thinking_parser:
        # ä½¿ç”¨è§£æå™¨å¤„ç†å®Œæ•´å†…?
        segments = thinking_parser.push_and_parse(full_content)
        final_segments = thinking_parser.flush()
        all_segments = segments + final_segments

        thinking_parts = []
        text_parts = []

        for segment in all_segments:
            if segment.type == SegmentType.THINKING:
                thinking_parts.append(segment.content)
            elif segment.type == SegmentType.TEXT:
                text_parts.append(segment.content)

        thinking_content = ''.join(thinking_parts)
        text_content = ''.join(text_parts)

    # å¤„ç† tool calls
    bracket_tool_calls = parse_bracket_tool_calls(full_content)
    all_tool_calls = parser.get_tool_calls() + bracket_tool_calls
    all_tool_calls = deduplicate_tool_calls(all_tool_calls)

    # æ„å»º content blocks
    content_blocks = []

    # æ·»åŠ  thinking blockï¼ˆå¦‚æœæœ‰?
    if thinking_content:
        content_blocks.append({
            "type": "thinking",
            "thinking": thinking_content
        })

    # æ·»åŠ  text blockï¼ˆå¦‚æœæœ‰?
    if text_content:
        content_blocks.append({
            "type": "text",
            "text": text_content
        })

    # æ·»åŠ  tool_use blocks
    for tc in all_tool_calls:
        func = tc.get("function") or {}
        tool_name = func.get("name") or ""
        tool_args_str = func.get("arguments") or "{}"
        tool_id = tc.get("id") or f"toolu_{generate_completion_id()[8:]}"

        try:
            tool_input = json.loads(tool_args_str)
        except json.JSONDecodeError:
            tool_input = {}

        content_blocks.append({
            "type": "tool_use",
            "id": tool_id,
            "name": tool_name,
            "input": tool_input
        })

    # ç¡®å®š stop_reason
    stop_reason = "tool_use" if all_tool_calls else "end_turn"

    # è®¡ç®— token ä½¿ç”¨?
    usage_info = _calculate_usage_tokens(
        full_content, context_usage_percentage, model_cache, model,
        request_messages, request_tools
    )
    input_tokens = usage_info["prompt_tokens"]
    completion_tokens = usage_info["completion_tokens"]

    if thinking_content:
        logger.debug(
            f"[Anthropic Usage with Thinking] {model}: input_tokens={input_tokens}, "
            f"output_tokens={completion_tokens}, thinking_chars={len(thinking_content)}"
        )
    else:
        logger.debug(
            f"[Anthropic Usage] {model}: input_tokens={input_tokens}, output_tokens={completion_tokens}"
        )

    return {
        "id": message_id,
        "type": "message",
        "role": "assistant",
        "content": content_blocks,
        "model": model,
        "stop_reason": stop_reason,
        "stop_sequence": None,
        "usage": {
            "input_tokens": input_tokens,
            "output_tokens": completion_tokens
        }
    }
