# -*- coding: utf-8 -*-

# GeekGate
# Based on kiro-openai-gateway by Jwadow (https://github.com/Jwadow/kiro-openai-gateway)
# Original Copyright (C) 2025 Jwadow
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""
é•¿æ–‡æ¡£åˆ†æ®µå¤„ç†å™¨?

å½“æ–‡æ¡£è¿‡é•¿æ—¶ï¼Œè‡ªåŠ¨å°†å…¶åˆ†æ®µå¤„ç†ï¼Œç„¶åæ‹¼æ¥ç»“æœ?
è¿™å¯ä»¥æœ‰æ•ˆé¿å…è¶…æ—¶é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹?Opus ç­‰æ…¢æ¨¡å‹?
"""

import re
from typing import List, Optional, Tuple
from loguru import logger

from geek_gateway.config import settings


# é»˜è®¤é…ç½®
DEFAULT_MAX_TOKENS_PER_CHUNK = 50000  # æ¯ä¸ªåˆ†æ®µçš„æœ€?token ?
DEFAULT_OVERLAP_TOKENS = 500  # åˆ†æ®µä¹‹é—´çš„é‡?token æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿è´¯ï¼‰
CHARS_PER_TOKEN_ESTIMATE = 4  # ä¼°ç®—ï¼šå¹³å‡æ¯?token ?4 ä¸ªå­—?


class ChunkedDocumentProcessor:
    """
    é•¿æ–‡æ¡£åˆ†æ®µå¤„ç†å™¨?

    å°†è¶…é•¿æ–‡æ¡£åˆ†æˆå¤šä¸ªè¾ƒå°çš„ç‰‡æ®µï¼Œä»¥é¿å…è¶…æ—¶é—®é¢˜?
    æ”¯æŒè‡ªå®šä¹‰åˆ†æ®µå¤§å°å’Œé‡å åŒºåŸŸ?
    """

    def __init__(
        self,
        max_tokens_per_chunk: int = DEFAULT_MAX_TOKENS_PER_CHUNK,
        overlap_tokens: int = DEFAULT_OVERLAP_TOKENS
    ):
        """
        åˆå§‹åŒ–åˆ†æ®µå¤„ç†å™¨?

        Args:
            max_tokens_per_chunk: æ¯ä¸ªåˆ†æ®µçš„æœ€?token ?
            overlap_tokens: åˆ†æ®µä¹‹é—´çš„é‡?token ?
        """
        self.max_tokens_per_chunk = max_tokens_per_chunk
        self.overlap_tokens = overlap_tokens
        self.max_chars_per_chunk = max_tokens_per_chunk * CHARS_PER_TOKEN_ESTIMATE
        self.overlap_chars = overlap_tokens * CHARS_PER_TOKEN_ESTIMATE

    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬?token æ•?

        ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°ä¼°ç®—ï¼Œæ¯”ç²¾ç¡®è®¡ç®—æ›´å¿?

        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬

        Returns:
            ä¼°ç®—?token ?
        """
        return len(text) // CHARS_PER_TOKEN_ESTIMATE

    def needs_chunking(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦éœ€è¦åˆ†æ®?

        Args:
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬

        Returns:
            å¦‚æœæ–‡æœ¬è¶…è¿‡é˜ˆå€¼åˆ™è¿”å› True
        """
        return len(text) > self.max_chars_per_chunk

    def find_split_point(self, text: str, target_pos: int) -> int:
        """
        åœ¨ç›®æ ‡ä½ç½®é™„è¿‘æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚?

        ä¼˜å…ˆåœ¨æ®µè½ã€å¥å­æˆ–å•è¯è¾¹ç•Œåˆ†å‰²ï¼Œä»¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€?

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            target_pos: ç›®æ ‡åˆ†å‰²ä½ç½®

        Returns:
            å®é™…åˆ†å‰²ä½ç½®
        """
        if target_pos >= len(text):
            return len(text)

        # æœç´¢èŒƒå›´ï¼šç›®æ ‡ä½ç½®å‰?500 å­—ç¬¦
        search_start = max(0, target_pos - 500)
        search_end = min(len(text), target_pos + 500)
        search_text = text[search_start:search_end]

        # ä¼˜å…ˆ?1ï¼šæ®µè½è¾¹ç•Œï¼ˆåŒæ¢è¡Œï¼‰
        paragraph_breaks = list(re.finditer(r'\n\n+', search_text))
        if paragraph_breaks:
            # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡ä½ç½®çš„æ®µè½è¾¹?
            best_match = min(paragraph_breaks, key=lambda m: abs((search_start + m.end()) - target_pos))
            return search_start + best_match.end()

        # ä¼˜å…ˆ?2ï¼šå¥å­è¾¹?
        sentence_breaks = list(re.finditer(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', search_text))
        if sentence_breaks:
            best_match = min(sentence_breaks, key=lambda m: abs((search_start + m.end()) - target_pos))
            return search_start + best_match.end()

        # ä¼˜å…ˆ?3ï¼šå•æ¢è¡Œ
        line_breaks = list(re.finditer(r'\n', search_text))
        if line_breaks:
            best_match = min(line_breaks, key=lambda m: abs((search_start + m.end()) - target_pos))
            return search_start + best_match.end()

        # ä¼˜å…ˆ?4ï¼šç©ºæ ¼ï¼ˆå•è¯è¾¹ç•Œ?
        word_breaks = list(re.finditer(r'\s+', search_text))
        if word_breaks:
            best_match = min(word_breaks, key=lambda m: abs((search_start + m.end()) - target_pos))
            return search_start + best_match.end()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œç›´æ¥åœ¨ç›®æ ‡ä½ç½®åˆ†?
        return target_pos

    def split_text(self, text: str) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªç‰‡æ®?

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬

        Returns:
            æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        if not self.needs_chunking(text):
            return [text]

        chunks = []
        current_pos = 0
        text_length = len(text)

        while current_pos < text_length:
            # è®¡ç®—è¿™ä¸ªåˆ†æ®µçš„ç»“æŸä½?
            chunk_end = current_pos + self.max_chars_per_chunk

            if chunk_end >= text_length:
                # æœ€åä¸€ä¸ªåˆ†?
                chunks.append(text[current_pos:])
                break

            # æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²?
            split_pos = self.find_split_point(text, chunk_end)

            # æå–åˆ†æ®µ
            chunk = text[current_pos:split_pos]
            chunks.append(chunk)

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªä½ç½®ï¼ˆè€ƒè™‘é‡å ?
            current_pos = split_pos - self.overlap_chars
            if current_pos <= 0 or current_pos >= split_pos:
                current_pos = split_pos  # é¿å…æ— é™å¾ªç¯

        logger.info(f"Split document into {len(chunks)} chunks")
        for i, chunk in enumerate(chunks):
            logger.debug(f"Chunk {i+1}: {len(chunk)} chars, ~{self.estimate_tokens(chunk)} tokens")

        return chunks

    def create_chunk_prompt(
        self,
        chunk: str,
        chunk_index: int,
        total_chunks: int,
        original_prompt: str
    ) -> str:
        """
        ä¸ºåˆ†æ®µåˆ›å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯?

        Args:
            chunk: æ–‡æ¡£ç‰‡æ®µ
            chunk_index: å½“å‰ç‰‡æ®µç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰
            total_chunks: æ€»ç‰‡æ®µæ•°
            original_prompt: åŸå§‹ç”¨æˆ·æç¤º?

        Returns:
            å¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        """
        if total_chunks == 1:
            return f"{original_prompt}\n\n{chunk}"

        context_info = f"[æ–‡æ¡£ç‰‡æ®µ {chunk_index + 1}/{total_chunks}]"

        if chunk_index == 0:
            instruction = "è¿™æ˜¯ä¸€ä¸ªé•¿æ–‡æ¡£çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚è¯·å¤„ç†è¿™éƒ¨åˆ†å†…å®¹ï¼Œåç»­ä¼šæä¾›å‰©ä½™éƒ¨åˆ†ã€?
        elif chunk_index == total_chunks - 1:
            instruction = "è¿™æ˜¯æ–‡æ¡£çš„æœ€åä¸€éƒ¨åˆ†ã€‚è¯·ç»“åˆä¹‹å‰çš„å†…å®¹å®Œæˆå¤„ç†ã€?
        else:
            instruction = f"è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ {chunk_index + 1} éƒ¨åˆ†ã€‚è¯·ç»§ç»­å¤„ç†ã€?

        return f"{context_info}\n{instruction}\n\n{original_prompt}\n\n---\n{chunk}\n---"

    def merge_responses(self, responses: List[str]) -> str:
        """
        åˆå¹¶å¤šä¸ªåˆ†æ®µçš„å“åº?

        Args:
            responses: å„åˆ†æ®µçš„å“åº”åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„å®Œæ•´å“åº”
        """
        if len(responses) == 1:
            return responses[0]

        # ç®€å•æ‹¼æ¥ï¼Œç”¨åˆ†éš”ç¬¦è¿æ¥
        merged = "\n\n".join(responses)

        logger.info(f"Merged {len(responses)} responses into one")
        return merged


def extract_document_from_messages(messages: List[dict]) -> Tuple[Optional[str], int]:
    """
    ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–å¯èƒ½çš„é•¿æ–‡æ¡£å†…å®¹?

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        (æ–‡æ¡£å†…å®¹, æ¶ˆæ¯ç´¢å¼•) ?(None, -1)
    """
    for i, msg in enumerate(messages):
        content = msg.get("content", "")
        if isinstance(content, str) and len(content) > DEFAULT_MAX_TOKENS_PER_CHUNK * CHARS_PER_TOKEN_ESTIMATE:
            return content, i
        elif isinstance(content, list):
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    text = block.get("text", "")
                    if len(text) > DEFAULT_MAX_TOKENS_PER_CHUNK * CHARS_PER_TOKEN_ESTIMATE:
                        return text, i

    return None, -1


# å…¨å±€å®ä¾‹
chunked_processor = ChunkedDocumentProcessor()
